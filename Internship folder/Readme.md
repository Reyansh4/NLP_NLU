# **OVERVIEW**
This Repository contains the work that I have done in Internship with Suvidha Foundation
</br>

# **DETAILS**
In this Internship i worked on the Following
</br>
 **Research paper:-** [Link](https://arxiv.org/pdf/1704.04368.pdf)
 **Statement:-** Text Summarization for News Articles </br>
 **Dataset:-** Dailymail/CNN Dataset </br>
 **Description** </br>
 In this repository I created two type of Models</br>
 1. A Sequence to Sequence Attention Model (For Abstractive Summarization)</br>
 2. A Pretrained BERT Model/Library (For Extractive Summarization)</br>

* The req_attachments folder has the model the abstractive summmarizer an .ipynb file for the code file 
* The model has encoder and decoder model differently stored and the custom trained tokeizer for the model.
</br>
* Follow the steps in the .ipynb to get the custom model for your required dataset. Ultimately check for the model on the custom text or compare the rouge score.
</br>
* The BERT_EXTSUM_Model.ipynb file has the code related to BERT Extractive summarizer
</br>
* My technical Understandings from the Research Paper and the Model outputs are scripted in the Technical Report.pdf file